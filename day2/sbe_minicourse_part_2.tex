\documentclass[aspectratio=169, xcolor=dvipsnames, 11pt]{beamer}
\usetheme{moloch}
\molochset{
  progressbar = frametitle,
  block = fill
}
% Use beamer's page number in head/foot template
\setbeamertemplate{footline}[frame number]
\setbeamersize{text margin left=0.5cm,text margin right=0.5cm}

% from https://tex.stackexchange.com/a/441705/36296
\makeatletter
\addtobeamertemplate{date}{\centering}{}
\addtobeamertemplate{institute}{\centering}{}
\patchcmd{\beamer@@tmpl@title}{\raggedright}{\centering}{}{}
\patchcmd{\beamer@@tmpl@author}{\raggedright}{\centering}{}{}
\setlength{\moloch@titleseparator@linewidth}{2pt}
\setlength{\moloch@progressinheadfoot@linewidth}{1.5pt}
\setbeamercolor{progress bar}{fg=FGVBlue, bg=white}
\makeatother

% ----- Packages -----
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{xcolor} % For colored bullets
\usepackage{appendixnumberbeamer}
\usepackage{emoji}
\setemojifont{Apple Color Emoji}
% TikZ for decorative braces in slides
\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing,positioning,calc}

% ----- Metadata -----
\title[]{ML in Economics and Finance: Where do We Go Now? - Part II}

\author[]{
  \textbf{Raul Riva}
}
\institute[]{
  \textcolor{FGVBlue}{FGV EPGE}
  \vspace{0.75cm}
}
\date[]{December, 2025 \\ \vspace{1cm} INSPER - SÃ£o Paulo}

% ----- Custom commands -----
\newcommand{\R}{{\mathbb{R}}}
\renewcommand{\Pr}{{\mathbb{P}}}
\newcommand{\E}{{\mathbb{E}}}
\newcommand{\var}{{\mathrm{var}}}
\newcommand{\s}{{\boldsymbol{s}}}
\newcommand{\cc}{{\boldsymbol{c}}}
\newcommand{\uu}{{\boldsymbol{u}}}
\newtheorem{prop}[theorem]{Proposition}

% ----- Defining colors -----
\usepackage[dvipsnames]{xcolor}
\definecolor{slateblue}{RGB}{45, 62, 80}
\definecolor{KelloggPurple}{HTML}{4E2A84}
\setbeamercolor{palette primary}{bg=slateblue, fg=white}
\definecolor{FGVBlue}{RGB}{0, 114, 188}
\setbeamercolor{title separator}{bg=FGVBlue, fg=FGVBlue}
\setbeamercolor{background canvas}{bg=white}

% ----- Hyperlinks and Bibliography -----
\usepackage[backend=biber,style=authoryear,maxcitenames=2,mincitenames=1,maxbibnames=99,uniquename=false,uniquelist=false]{biblatex}
\ExecuteBibliographyOptions{maxcitenames=2,mincitenames=1,uniquelist=false}
\addbibresource{../references.bib}
\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=FGVBlue,
  urlcolor=FGVBlue,
  citecolor=FGVBlue
} % close \hypersetup
\DeclareFieldFormat{labelyear}{#1}
\DeclareFieldFormat{shorthand}{#1}
\DeclareFieldFormat{extraalpha}{#1}
\AtEveryCitekey{\color{FGVBlue}}
\DeclareNameFormat{labelname}{%
    \nameparts{#1}%
    \usebibmacro{name:family}{\namepartfamily}{\namepartgiven}{\namepartprefix}{\namepartsuffix}%
    \usebibmacro{name:andothers}}
\let\cite\textcite
\renewcommand*{\bibfont}{\tiny}

% Make section titles slateblue
\makeatletter
% Preserve starred sections and optional arguments while coloring section titles.
\let\oldsection\section
\renewcommand{\section}{\@ifstar\star@section\@section}
\newcommand{\@section}[2][]{%
    \if\relax\detokenize{#1}\relax
        \oldsection{\textcolor{slateblue}{#2}}%
    \else
        \oldsection[#1]{\textcolor{slateblue}{#2}}%
    \fi
}
\newcommand{\star@section}[1]{\oldsection*{\textcolor{slateblue}{#1}}}
\makeatother


%---------------- Document Start ----------------%
\begin{document}
\begin{frame}[plain]
    \titlepage
\end{frame}

\begin{frame}{Flight Plan {\normalfont\emoji{airplane}}}
    \begin{enumerate}
        \item What is ML, anyway?\tikz[remember picture,baseline] \node (it1) {};
        \item Causality in High Dimensions\tikz[remember picture,baseline] \node (it2) {};
        \item (Seriously) Heterogeneous Partial Effects\tikz[remember picture,baseline] \node (it3) {};
        \item Solving Large-Scale General Equilibrium Models\tikz[remember picture,baseline] \node (it4) {};
    \end{enumerate}

    % Decorative braces indicating Today / Tomorrow on the right
    \begin{tikzpicture}[remember picture,overlay]
        \coordinate (todayAnchor) at ($(it2.east)+(0.45cm,-0.2cm)$);
        \coordinate (tomorrowAnchor) at ($(it4.east)+(0.45cm,-0.2cm)$);

        \draw[decorate,decoration={brace,amplitude=6pt,mirror},line width=1pt]
            (todayAnchor |- it2.south) -- ($(todayAnchor |- it1.north)+(0,0.3cm)$)
            node[midway,right=12pt]{\textbf{Yesterday}};

        \draw[decorate,decoration={brace,amplitude=6pt,mirror},line width=1pt]
            (tomorrowAnchor |- it4.south) -- ($(tomorrowAnchor |- it3.north)+(0,0.3cm)$)
            node[midway,right=12pt]{\textbf{Today}};
    \end{tikzpicture}
\end{frame}

\section{Heterogeneous Partial Effects}

\begin{frame}{Motivation}
Let $Y$ be an outcome and $X, Z$ be features (covariates). We frequently want to approximate
\begin{equation*}
    h(x, z) \equiv \E[Y | X=x, Z=z]
\end{equation*}
and the \textbf{partial effects}
\begin{equation*}
    \frac{\partial}{\partial x} h(x, z) = \frac{\partial}{\partial x} \E[Y | X=x, Z=z].
\end{equation*}
\begin{itemize}
    \pause
    \item This is a prediction problem after all!
    \pause
    \item Approach 1: impose a parametric model for $h$, e.g. linear regression. Pros and cons?
    \item Approach 2: use fully nonparametric methods. Pros and cons?
    \pause
    \item The third way is the charm: a bit of structure, a bit of ML!
\end{itemize}
\end{frame}

\begin{frame}{Example I - Heterogenous Treatment Effects}
\begin{itemize}
    \item Outcomes $Y_i$ depend on a treatment $X_i \in \R$ and covariates $Z_i \in \R^p$;
    \item The dose $X_i$ depends on observables $Z_i$;
    \item Potential outcomes $Y_i(x)$ for each dose $x \in \R$;
    \pause
    \item The conditional average effect of increasing the dose is $\tau(x, z) \equiv \frac{\partial}{\partial x} \E[Y(x) | Z=z]$;
    \item If $\E[Y(x) | X=x, Z=z] = \E[Y(x) | Z=z]$ (common assumption in the literature), then 
    \[
    \tau(x, z) = \frac{\partial}{\partial x} \E[Y | X=x, Z=z] = \frac{\partial h(x,z)}{\partial x}
    \]
\end{itemize}
\end{frame}

\begin{frame}{Example II - Grouped Heterogeneity}
Consider the following model:
\begin{equation*}
    Y_i = \alpha(Z_i) +  X_i' \beta + \varepsilon_i, \quad \E[\varepsilon_i | X_i, Z_i] = 0,  
\end{equation*}
\begin{itemize}
    \item $X_i$ affects $Y_i$ homogeneously;
    \item Intercept $\alpha(Z_i)$ varies with $Z_i$, maybe in a highly nonlinear way;
    \item Since $Z_i$ and $X_i$ can be correlated, this can affect inference about $\beta$;
    \item \cite{Bonhomme2015} studied how democracy affects national income using this model;
    \item In their case: $\alpha(Z_i)$ is constant across groups but $Z_i$ defines membership;
    \item In our notation: $h(z, x) = \alpha(z) + x' \beta$
\end{itemize}
\end{frame}

\begin{frame}{How can we balance flexibility and interpretability?}
\cite{masini_balancing_2025} \emoji{flag-brazil} proposed a middle ground:
\begin{equation*}
    h(x, z) = x^\top \beta(z), \qquad \frac{\partial h(x,z)}{\partial x} = \beta(z)
\end{equation*}
\begin{itemize}
    \item The partial effect of $X$ on $Y$ varies with $Z$ through $\beta(z)$;
    \item $\beta(.)$ is a Lipschitz function that can be estimated with ML methods;
    \item No need to numerically approximate $\frac{\partial}{\partial x} h(x, z)$;
    \pause
    \item Explicit conditions for consistency \textit{and} asymptotic normality of $\hat{\beta}(z)$;
    \pause
    \item Secrete sauce: a variant of the \textbf{Random Forest} algorithm!
\end{itemize}
\pause
\vspace{0.5cm}
But what is a Random Forest, anyway? \emoji{thinking-face}
\end{frame}

\begin{frame}[standout]
    Questions?
\end{frame}

\section{Quick Intro to Random Forests}

\begin{frame}{Random Trees}
Recall the general ML framework:
\begin{equation*}
    Y = f(X) + \varepsilon
\end{equation*}
A \textbf{Random Tree} is a particular way of parametrizing $f$!
\pause
\vspace{0.5cm}
\begin{itemize}\itemsep0.4em
    \item If $X \in \R^p$, consider a finite partition $\{S_1, S_2, \ldots, S_m\}$ of $\R^p$;
    \item Each $S_i$ is a hyperrectangle defined by recursive binary splits on the covariates;
    \item On each $S_i$, $f$ is constant: $f(x) = \mu_i$ for all $x \in S_i$;
    \item After a tree has been estimated (``grown''), $\hat{f}(x_i) = \mu_i$ if $x_i \in S_i$;
\end{itemize}
\pause
\vspace{0.4cm}
The really complicated part: there are \textit{so many} partitions... how to pick one?

\vspace{0.3cm}
\begin{center}
    \fbox{\parbox{0.9\textwidth}{
    \cite{Hyafil1976}: this is harder than you think! The problem is NP-complete!
}}
\end{center}

\end{frame}

\begin{frame}{Example: Predicting Baseball Player Salaries}
    \begin{columns}[T,onlytextwidth]
        \column{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../pictures/Baseball.png}

        \column{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../pictures/Baseball_region.png}
    \end{columns}
\end{frame}

\begin{frame}{How to pick a split point? Use some greed!}
Let's say you want to split on feature $X_j$ at point $\delta$:
\begin{gather*}
    S_1 \equiv \{x \in \R^p: x_j \leq \delta\}, \quad S_2 \equiv \{x \in \R^p: x_j > \delta\}\\
    \mu_1 \equiv \sum_{i: x_i \in S_1} \frac{Y_i}{n_1}, \quad \mu_2 \equiv \sum_{i: x_i \in S_2} \frac{Y_i}{n_2}
\end{gather*}
\pause
\begin{itemize}\itemsep0.4em
    \item Define $SSR(\delta) \equiv \sum_{i: x_i \in S_1} (Y_i - \mu_1)^2 + \sum_{i: x_i \in S_2} (Y_i - \mu_2)^2$
    \item Choose $\delta$ to minimize $SSR(\delta) \implies$ this is usually very fast to compute!
    \pause
    \item Repeat this for all features $X_j$ and pick the best one;
    \item Important: you need some stopping rule! There is a huge literature on this... 
    \item Example: minimum number of observations per leaf;
\end{itemize}

\pause
\vspace{0.5cm}
This is the so-called the \textbf{CART} algorithm due to \cite{Breiman1984}.
\end{frame}

\begin{frame}{How to get a Random Forest?}

A Random Forest is an \textbf{ensemble} of Random Trees:
\[
\hat{f}^{(1)}(x), \hat{f}^{(2)}(x), \ldots, \hat{f}^{(B)}(x)
\]

\vspace{0.3cm}
\pause
Each tree is grown on a \textbf{perturbed version} of the data:
\begin{itemize}\itemsep0.2em
    \item Bootstrap sample of the observations;
    \item Random subset of features considered at each split;
\end{itemize}

\pause
\vspace{0.3cm}
The forest prediction is the \textbf{average}:
\[
\hat{f}_{RF}(x) = \frac{1}{B} \sum_{b=1}^B \hat{f}^{(b)}(x)
\]

\pause
\vspace{0.3cm}
Key insight:
\begin{itemize}\itemsep0.4em
    \item Each tree is noisy and biased, but averaging them reduces variance dramatically;
    \item Randomness \emph{decorrelates} the trees, making averaging powerful;
\end{itemize}
\end{frame}

\begin{frame}[standout]
    Questions?
\end{frame}

\section{Back to Partial Effects}

\begin{frame}{The Main Insight}
We have a random sample $\{(Y_i, X_i, Z_i)\}_{i=1}^n$ from
\begin{equation*}
    Y_i = X_i^\top \beta(Z_i) + \varepsilon_i, \quad \E[\varepsilon_i | X_i, Z_i] = 0
\end{equation*}
\vspace{-0.3cm}
\pause
\begin{itemize}
    \item \cite{masini_balancing_2025} proposed to estimate $\beta(z)$ using a modified Random Forest;
    \item Key modification: at each split, try to minimize the \textbf{local least squares} criterion;
\end{itemize}
\pause
\vspace{0.4cm}
Suppose you want to split at $Z_j \leq \delta$ as before. Then:
\begin{gather*}
    S_1 \equiv \{z \in \mathbb{R}^p: z_{j} \leq \delta\}, \quad S_2 \equiv \{z \in \mathbb{R}^p: z_{j} > \delta\}\\
    (\hat{\beta}_1) \equiv \arg\min_{\beta} \sum_{i: Z_{i,j} \in S_1} (Y_i - X_i^\top \beta)^2, \quad (\hat{\beta}_2) \equiv \arg\min_{\beta} \sum_{i: Z_{i,j} \in S_2} (Y_i - X_i^\top \beta)^2\\
    SSR(\delta) \equiv \sum_{i: Z_{i,j} \in S_1} (Y_i - X_i^\top \hat{\beta}_1)^2 + \sum_{i: Z_{i,j} \in S_2} (Y_i - X_i^\top \hat{\beta}_2)^2
\end{gather*}
\pause
Pick $\delta$ to minimize $SSR(\delta)$!
\end{frame}

\begin{frame}{The Algorithm}
Pick a number of trees $B$ and a minimum leaf size $k$. For $b=1, \ldots, B$:
\vspace{0.3cm}
\begin{enumerate}
    \item Draw a bootstrap sample of size $s \leq n$ from the data;
    \item Divide the data into two halves $\mathcal{A}$ and $\mathcal{B}$;
    \item Using $\mathcal{B}$, keep splitting at random dimensions $j$ using the previous criterion;
    \item Stop when all leaves have less than $2k -1$ and more than $k$ observations;
    \item Using $\mathcal{A}$, estimate $\beta(z)$ using only observations in the leaf where $z$ falls;
\end{enumerate}
\vspace{0.5cm}
The final estimate is
\begin{equation*}
    \hat{\beta}(z) = \frac{1}{B} \sum_{b=1}^B \hat{\beta}^{(b)}(z)
\end{equation*}
\pause
\vspace{0.8cm}
This algorithm uses \textbf{honest trees}! Similar intuition to cross-fitting.
\end{frame}

\begin{frame}{Cool Properties and Limitations}
\textbf{Cool properties}:
\begin{itemize}
        \item Highly interpretable and relatively mild assumptions on $\beta(z)$;
        \item Easy confidence intervals for $\beta(z)$ at any point $z$:
        \begin{equation*}
            \Omega^{-1/2}(z) \left( \hat{\beta}(z) - \beta(z) \right) \xrightarrow{d} \mathcal{N}(0, I_q)
        \end{equation*}
        for some complicated $\Omega(x)$ that can be estimated consistently;
        \item There is also a Lagrange multiplier test for homogeneity of $\beta(z)$;
    \end{itemize}
\vspace{0.3cm}
\pause
\textbf{Limitations}:
\begin{itemize}
        \item The dimension of $X_i$ should be small relative to $n$;
        \item The dimension of $Z_i$ cannot be \textit{that} large relative to $n$;
        \item Pointwise inference only;
        \item It cannot be readily applied to time series and panel data;
        \item It can be computationally demanding in large datasets;
    \end{itemize}
\end{frame}

\begin{frame}[standout]
    \begin{center}
       Questions?
    \end{center}
\end{frame}

\begin{frame}[standout]
    \begin{center}
       Take a deep breath... we are changing topics!\\
       \vspace{0.5cm}
       \emoji{grimacing-face}
    \end{center}
\end{frame}

\section{Solving Large Models Through Deep Learning}

\begin{frame}{There is tension in the air...}
    \begin{itemize}
        \item Dynamic programming/stochastic control everywhere: Macro, Finance, IO, Labor, etc;
        \item Pen and paper won't cut it: we need sophisticated numerical methods;
        \item \underline{Tension}: \alert{the models we can solve} vs \textcolor{FGVBlue}{the ones we \textit{would like} to solve};
        \item Main challenge: computational cost increases exponentially with state space dimension;
        \item Traditional practice: simpler economics, simpler models;
        \pause
        \item In practice: forget solving models with more than $\approx$ 8-10 state variables;
    \end{itemize}
    \vspace{0.6cm}
    \begin{center}
    \fbox{\parbox{0.5\textwidth}{
        \centering
    \textbf{Deep Learning} to the rescue!
}}
\end{center}
\end{frame}

\begin{frame}{The Three Musketeers}
Let $\s\in \R^n$ be a state vector, $\cc \in \R^m$ be a control vector, and $\uu \in \R^k$ be shocks:
    \begin{gather*}
        V(\s) = \max_{c} \left\{ u(\cc) + e^{-\rho} \E\left[ V\left(\s' \right) \mid \s,\cc \right] \right\}\\
        \s' = f(\s,\cc,\uu), \quad \uu \sim F(\cdot)
    \end{gather*}
\pause
    Following \cite{Powell2011}
    \begin{itemize}
        \item \textbf{The curse of representation}: a Cartesian grid for $\s$ grows exponentially with $n$;
        \begin{itemize}
            \item Ex: 100 grid points per dimension $\implies$ $10^{2n}$ total points;
            \item Ex: $n=26$, 100 points per dimension $= O(10^{40})$ TB of RAM (AWS $\approx 10^9$ TB);
        \end{itemize}
        \item \textbf{The curse of expectation}: $$\E\left[ V\left(\s' \right) \mid \s,\cc \right] = \int_{u_1} \int_{u_2}\cdots\int_{u_k} V\left( f(\s,\cc(\s),\uu) \right) \phi(\uu)d\uu_1 d\uu_2\cdots d\uu_k $$
        \item \textbf{The curse of optimization}: Given $V(.)$,  $10^{2n}$ optimizations to find $\cc(\s)$;
    \end{itemize}
    
\end{frame}

\begin{frame}{Can Ito Calculus Save Us?}
\begin{gather*}
    V(\s_t) = \sup_{\cc}\E_t\left\{\int\limits_{t}^{\infty}e^{-\rho(\nu - t)} u(\cc_\nu) d\nu \right\}\\
    \text{subject to } d\s_t = f(\s_t, \cc_t)dt + g(\s_t, \cc_t)d\mathbf{B}_t
\end{gather*}
\pause
The Stochastic Control gang is smart... give up $\E$, use Ito's lemma, and solve a PDE!

\pause
\vspace{0.4cm}
Under some conditions, $V(\s)$ and $\cc(\s)$ are such that:
\begin{gather*}
    HJB(\s, \cc, V) \equiv -\rho V(\s) + u(\cc) + \nabla_{\s} V(\s)^\top f(\s, \cc) + \frac{1}{2} \text{tr}\left( g(\s, \cc)^\top\nabla^2_{\s\s} V(\s) g(\s, \cc)  \right)\\
    \pause
    HJB(\s, \cc(s), V) = 0, \quad \forall \s \in \R^n \, \text{at the optimum policy}\\
    \cc(s) = \arg \max_{\cc} u(\cc) + \nabla_{\s} V(\s)^\top f(\s, \cc) + \frac{1}{2} \text{tr}\left( g(\s, \cc) g(\s, \cc)^\top \nabla^2_{\s\s} V(\s) \right)
\end{gather*}
\end{frame}

\begin{frame}{Good News and Bad News}
\begin{itemize}
    \item \textbf{Good news}: no expectation operator! Solve the PDE and we are done;
    \pause
    \item \textbf{Bad news}: we still need to represent $V(\s)$ and $\cc(\s)$ somehow;
    \pause
    \item \textbf{Worse news}: we still need to optimize $\cc(\s)$ at each $\s$;
    \pause
    \item \textbf{Even worse news}: solving PDEs in high dimensions is notoriously difficult;
    \pause
    \item Typical approach from engineering: \textbf{Finite Differences} or \textbf{Finite Elements};
    \item Let $n=1$ (scalar state) and an equally spaced grid $\{\s_1, \s_2, \ldots, \s_M\}$. Then:
    \begin{equation*}
        \frac{dV(s)}{ds} \Big|_{s=s_i} \approx \frac{V(s_{i+1}) - V(s_{i-1})}{2\Delta s}, \quad \frac{d^2V(s)}{ds^2} \Big|_{s=s_i} \approx \frac{V(s_{i+1}) - 2V(s_i) + V(s_{i-1})}{\Delta s^2}
    \end{equation*}
    \pause
    \item Drawback: we need very fine grids to approximate derivatives well;
\end{itemize}
\end{frame}

\begin{frame}{The Beauty of Spectral Methods}
\textbf{Key idea}: Parametrize $V(\s)$ using a convenient \textit{basis of functions}!
\pause
\vspace{0.4cm}
\begin{itemize}
    \item Example for $n=1$:
    \[
        V(s) = \sum_{k=0}^{K} a_k T_k(s)
    \]
    \pause
    \item Derivatives of $V(s)$ are analytically available
    \quad \textit{(fast... like really fast!)};
    \pause
    \item Excellent global approximation properties
    \quad (minimax interpolation error);
    \pause
    \item \textbf{However}: if $n>1$, the basis relies on \textbf{tensor products}:
    \[
        V(s_1,\ldots,s_n)
        =
        \sum_{k_1=0}^{K}\cdots\sum_{k_n=0}^{K}
        a_{k_1,\ldots,k_n}
        \prod_{j=1}^n T_{k_j}(s_j)
    \]
    \pause
    \item Number of coefficients grows exponentially with $n$:
    \[
        (K+1)^n
    \]
\end{itemize}
\end{frame}

\begin{frame}{A Deep Contribution}
\begin{itemize}\itemsep0.5em
    \item Chebyshev polynomials are amazing but only in lower dimensions (say $n < 8$);
    \pause
    \item A good basis of functions for us should feature:
    \begin{itemize}
        \item \textbf{Richness}: be able to approximate a wide variety of functions;
        \item \textbf{Performance}: Fast computation derivatives/gradients;
        \item \textbf{Expressiveness}: Few parameters should suffice to approximate complex functions;
    \end{itemize}
\end{itemize}

\vspace{0.8cm}

\cite{Duarte2024}: let's embrace Neural Networks!
\vspace{0.4cm}
\pause

\textbf{But why?}
\begin{itemize}
    \item Neural networks can approximate almost any function;
    \item Computing gradients is super fast due to breakthroughs in \textit{Automatic Differentiation};
    \item If we use \textit{deep} Neural Nets, we need few parameters!
\end{itemize}
\end{frame}

\begin{frame}[standout]
    \begin{center}
       Questions?
    \end{center}
\end{frame}

\section{But what is a Neural Network?}
\begin{frame}{A Smart Way of Composing Functions}
    \begin{columns}[T,onlytextwidth]
        \column{0.54\textwidth}
        \begin{itemize}\itemsep0.8em
            \item Each node is equipped with weights $\boldsymbol{w}$, a bias $b$, and an activation function $\sigma(.)$;
            \item Hidden nodes compute $\sigma(\boldsymbol{w}^\top \boldsymbol{x} + b)$;
            \item Examples of $\sigma(.)$: $\underbrace{\max\{0, x\}}_{\text{ReLU}}$, $\tanh(x)$;
            \item Output of hidden layer: $\boldsymbol{h}(\boldsymbol{x}, \boldsymbol{\theta}) = \sigma\left(W \boldsymbol{x} + \boldsymbol{b}\right)$;
            \item Final output: $\hat{y} = \boldsymbol{w}_{out}^\top \boldsymbol{h}(\boldsymbol{x}, \boldsymbol{\theta}) + b_{out}$;
            \item Abuse notation: $\boldsymbol{\theta} \equiv(\boldsymbol{W}, \boldsymbol{b}, \boldsymbol{w}_{out},b_{out})$
            \item \textit{Shallow neural network} = 1 hidden layer;
        \end{itemize}

        \column{0.45\textwidth}
        \centering
        \begin{figure}
            \centering
            \includegraphics[width=0.95\textwidth]{../pictures/single_layer_NN.png}
        \end{figure}
    \end{columns}
\end{frame}

\begin{frame}{Does it have good properties?}
    \cite{Cybenko1989} proved that this basis of functions is rich:
    \begin{theorem}[Universal Approximation]
        Any continuous function on a compact subset of $\mathbb{R}^n$ can be approximated to arbitrary precision by a shallow neural network with any non-polynomial activation function.
    \end{theorem}
    \pause
    Some bad news: how many hidden nodes do we need?
    \pause
    \pause
    \noindent\rule{\linewidth}{0.4pt}
    Some \textbf{great} news at last:
    \begin{itemize}
        \item If we allow for \textit{more layers}, we can use fewer nodes in each layer;
        \item More layers and fewer nodes $=$ fewer parameters than a single super wide layer;
    \end{itemize}
    \pause
    \begin{theorem}[\cite{Lu_NIPS}]
        For any Lebesgue integrable function $f \in L^1 (\mathbb{R}^n)$, there exists a ReLU deep neural network with width at most $n + 4$ in every hidden layer that approximates $f$ to arbitrary precision.
    \end{theorem}
\end{frame}

\begin{frame}{What is a Deep Neural Network?}
    \begin{figure}
        \centering
        \includegraphics[width=0.85\textwidth]{../pictures/multi_layer_NN.png}
    \end{figure}
    
\end{frame}

\begin{frame}{How do we find these parameters?}
\textit{Training the network} = picking $\boldsymbol{\theta}$ to minimize some loss function:
\begin{equation*}
    \mathcal{L}(\boldsymbol{\theta}) = \frac{1}{n} \sum_{i=1}^n \left(y_i - f(\boldsymbol{x}_i, \boldsymbol{\theta})\right)^2
\end{equation*}
\pause

One scheme to minimize $\mathcal{L}(\boldsymbol{\theta})$ is Gradient Descent:
\begin{equation*}
    \nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta}) = -\frac{2}{n} \sum_{i=1}^n \left(y_i - f(\boldsymbol{x}_i, \boldsymbol{\theta})\right) \nabla_{\boldsymbol{\theta}} f(\boldsymbol{x}_i, \boldsymbol{\theta})
\end{equation*}
\pause
\begin{align*}
    \boldsymbol{\theta}_{k+1} &= \boldsymbol{\theta}_k - \gamma \nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta}_k)\\
    &= \boldsymbol{\theta}_k + \frac{2\gamma}{n} \sum_{i=1}^n \left(y_i - f(\boldsymbol{x}_i, \boldsymbol{\theta}_k)\right) \nabla_{\boldsymbol{\theta}} f(\boldsymbol{x}_i, \boldsymbol{\theta}_k)\\
    &= \boldsymbol{\theta}_k + \tilde{\gamma} \sum_{i=1}^n \hat{\varepsilon}_i(\boldsymbol{\theta}_k) \nabla_{\boldsymbol{\theta}} f(\boldsymbol{x}_i, \boldsymbol{\theta}_k)
\end{align*}
\end{frame}

\begin{frame}[standout]
    \begin{center}
       Questions?
    \end{center}
\end{frame}

\section{Back to Solving Models!}


\begin{frame}[standout]
    \begin{center}
       Thank you!\\
        See you tomorrow, stay tuned!
    \end{center}
\end{frame}

% --- Appendix ---
\appendix

\begin{frame}[standout]
  \begin{center}
    \Huge
    Appendix and References
  \end{center}
\end{frame}

\begin{frame}{References}
    \footnotesize
    \printbibliography[heading=none]
\end{frame}

\end{document}
