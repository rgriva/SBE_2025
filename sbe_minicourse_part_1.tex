\documentclass[aspectratio=169, xcolor=dvipsnames, 11pt]{beamer}
\usetheme{moloch}
\molochset{
  progressbar = frametitle,
  block = fill
}
% Use beamer's page number in head/foot template
\setbeamertemplate{footline}[frame number]
\setbeamersize{text margin left=0.5cm,text margin right=0.5cm}

% from https://tex.stackexchange.com/a/441705/36296
\makeatletter
\addtobeamertemplate{date}{\centering}{}
\addtobeamertemplate{institute}{\centering}{}
\patchcmd{\beamer@@tmpl@title}{\raggedright}{\centering}{}{}
\patchcmd{\beamer@@tmpl@author}{\raggedright}{\centering}{}{}
\setlength{\moloch@titleseparator@linewidth}{2pt}
\setlength{\moloch@progressinheadfoot@linewidth}{1.5pt}
\setbeamercolor{progress bar}{fg=FGVBlue, bg=white}
\makeatother

% ----- Packages -----
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{xcolor} % For colored bullets
\usepackage{appendixnumberbeamer}
\usepackage{emoji}
\setemojifont{Apple Color Emoji}
% TikZ for decorative braces in slides
\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing,positioning,calc}

% ----- Metadata -----
\title[]{ML in Economics and Finance: Where do We Go Now? - Part I}

\author[]{
  \textbf{Raul Riva}
}
\institute[]{
  \textcolor{FGVBlue}{FGV EPGE}
  \vspace{0.75cm}
}
\date[]{December, 2025 \\ \vspace{1cm} INSPER - SÃ£o Paulo}

% ----- Custom commands -----
\newcommand{\R}{{\mathbb{R}}}
\renewcommand{\Pr}{{\mathbb{P}}}
\newcommand{\E}{{\mathbb{E}}}
\newcommand{\var}{{\mathrm{var}}}
\newtheorem{prop}[theorem]{Proposition}

% ----- Defining colors -----
\usepackage[dvipsnames]{xcolor}
\definecolor{slateblue}{RGB}{45, 62, 80}
\definecolor{KelloggPurple}{HTML}{4E2A84}
\setbeamercolor{palette primary}{bg=slateblue, fg=white}
\definecolor{FGVBlue}{RGB}{0, 114, 188}
\setbeamercolor{title separator}{bg=FGVBlue, fg=FGVBlue}
\setbeamercolor{background canvas}{bg=white}

% ----- Hyperlinks and Bibliography -----
\usepackage[backend=biber,style=authoryear]{biblatex}
\addbibresource{references.bib}
\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=FGVBlue,
  urlcolor=FGVBlue,
  citecolor=FGVBlue
} % close \hypersetup
\DeclareFieldFormat{labelyear}{#1}
\DeclareFieldFormat{shorthand}{#1}
\DeclareFieldFormat{extraalpha}{#1}
\DeclareNameFormat{default}{\namepartfamily}
\AtEveryCitekey{\color{FGVBlue}}
\let\cite\textcite
\renewcommand*{\bibfont}{\tiny}

% Make section titles slateblue
\makeatletter
% Preserve starred sections and optional arguments while coloring section titles.
\let\oldsection\section
\renewcommand{\section}{\@ifstar\star@section\@section}
\newcommand{\@section}[2][]{%
    \if\relax\detokenize{#1}\relax
        \oldsection{\textcolor{slateblue}{#2}}%
    \else
        \oldsection[#1]{\textcolor{slateblue}{#2}}%
    \fi
}
\newcommand{\star@section}[1]{\oldsection*{\textcolor{slateblue}{#1}}}
\makeatother


%---------------- Document Start ----------------%
\begin{document}

%---------------- Title Slide ----------------%
% Title page without margin changes
\begin{frame}[plain] % Use plain to avoid navigation symbols
  \titlepage
\end{frame}

\section{Intro}
%---------------- Intro ----------------%
\begin{frame}{Who is this guy?}
    \begin{itemize}\itemsep1em
        \item I have just joined \textcolor{FGVBlue}{FGV EPGE} as an Assistant Professor;
        \item I got my PhD in Finance at \textcolor{KelloggPurple}{Northwestern University};
        \item Asset Pricing + Macro-Finance + Econometrics;
        \end{itemize}
        \pause
        \vspace{1cm}
        \begin{center}
        \fbox{\parbox{0.9\textwidth}{\centering
        \alert{I am \textbf{not} an ML developer, but maybe a mildly sophisticated economist consumer}
        }}
        \end{center}
\end{frame}

\begin{frame}{Where are we?}
    \begin{itemize}\itemsep0.6em
            \item Last 20-30 years: explosion of computation power and popularization of ML techniques;
            \item Last 15 years: we economists imported several techniques from CS and Stats;
            \pause
            \item Many challenges in this translation:
            \vspace{-0.5cm}
                \begin{columns}[t]
                    \begin{column}{0.44\textwidth}
                        \begin{itemize}\itemsep0.3em
                            \item Causality vs pattern recognition;
                            \item Interpretability;
                        \end{itemize}
                    \end{column}
                    \begin{column}{0.44\textwidth}
                        \begin{itemize}\itemsep0.5em
                            \item Sophisticated notions of equilibrium;
                            \item Time series dynamics;
                        \end{itemize}
                    \end{column}
                \end{columns}
        \end{itemize}
    \pause
    \vspace{0.75cm}
    \textbf{Right now}:
    \begin{itemize}\itemsep0.6em
        \item Better understanding of the limitations of "plug and play" ML;
        \item[\textcolor{ForestGreen}{\bullet}] Great stuff: new hybrid methods designed by and for economists;
        \item[\textcolor{Red}{\bullet}] Bad stuff: we are flooded with tutorials, books, videos, bootcamps...
    \end{itemize}   
\end{frame}

\begin{frame}{Where do we go now?}
    \begin{itemize}
        \item The Econ/Finance forecasting crowd was really fast in adopting ML...
        \item But what else? What is worth knowing about ML in Econ and Finance?
    \end{itemize}
\pause
\vspace{0.5cm}
    \textbf{What I will do}:
    \begin{itemize}
        \item My own economist-crafted definition of ML methods and how to think about them;
        \item Three very cool agendas where ML can help economists
        \item Causality in HD, seriously heterogeneous treatment effects, and solving large models;
    \end{itemize}
\pause
\vspace{0.5cm}
    \textbf{What I will \alert{\underline{not}} do}:
    \begin{itemize}
        \item Teach you how to code;
        \pause
        \item Pretend I know how to prove the complicated theorems and walk you through proofs;
        \pause
        \item Lie to you and say you can easily perform any of this in Stata! \emoji{face-with-rolling-eyes} 
    \end{itemize}
\end{frame}

\begin{frame}{Who is this for?}
    \begin{itemize}
        \item Students starting their empirical research agendas;
        \item Fellow empirical researchers trying to grasp what kind of ML tools might be useful;
        \item Someone coming from Stats or CS into Economics;
    \end{itemize}
\pause
\vspace{0.5cm}
\textbf{Who is this \textcolor{red}{\underline{not}} for?}
\begin{itemize}
    \item Super sophisticated economists already deploying these techniques everywhere;
    \item Hardcore econometricians looking for open theoretical problems;
\end{itemize}
\pause
\vspace{0.5cm}
\begin{center}
        \fbox{\parbox{0.99\textwidth}{\centering
        DISCLAIMER: These are \textbf{my} own views, based on \textbf{my} experience, and \textbf{my} own readings.\\ Other people will disagree.
        }}
\end{center}

\end{frame}

\begin{frame}{Flight Plan {\normalfont\emoji{airplane}}}
    \begin{enumerate}
        \item What is ML, anyway?\tikz[remember picture,baseline] \node (it1) {};
        \item Causality in High Dimensions\tikz[remember picture,baseline] \node (it2) {};
        \item (Seriously) Heterogeneous Treatment Effects\tikz[remember picture,baseline] \node (it3) {};
        \item Solving Large-Scale General Equilibrium Models\tikz[remember picture,baseline] \node (it4) {};
    \end{enumerate}

    % Decorative braces indicating Today / Tomorrow on the right
    \begin{tikzpicture}[remember picture,overlay]
        \coordinate (todayAnchor) at ($(it2.east)+(0.45cm,-0.2cm)$);
        \coordinate (tomorrowAnchor) at ($(it4.east)+(0.45cm,-0.2cm)$);

        \draw[decorate,decoration={brace,amplitude=6pt,mirror},line width=1pt]
            (todayAnchor |- it2.south) -- ($(todayAnchor |- it1.north)+(0,0.3cm)$)
            node[midway,right=12pt]{\textbf{Today}};

        \draw[decorate,decoration={brace,amplitude=6pt,mirror},line width=1pt]
            (tomorrowAnchor |- it4.south) -- ($(tomorrowAnchor |- it3.north)+(0,0.3cm)$)
            node[midway,right=12pt]{\textbf{Tomorrow}};
    \end{tikzpicture}
    \pause
    \vspace{0.5cm}
    \begin{center}
        	\textbf{\textcolor{FGVBlue}{Please bring questions at any time!}}
    \end{center}
\end{frame}

\begin{frame}[standout]
    Questions?
\end{frame}

\section{A General Framework}
\begin{frame}{What is \textit{Machine Learning}?}
    \begin{itemize}\itemsep0.6em
        \item Different fields = different definitions: CS, Stats, Operations Research, ...
        \item Many types: Supervised, Unsupervised, Reinforcement Learning, Deep Learning, ...
        \item More buzzwords = better consulting gigs! \emoji{money-mouth-face}
        \pause
        \item Today and tomorrow: \textbf{Supervised Learning};
        \pause
        \item I will be brave enough and provide the one I think is really useful for Economists:
    \end{itemize}
    \pause
    \vspace{0.6cm}
    \begin{center}
        \fbox{\parbox{0.95\textwidth}{\centering
        \alert{(Supervised) \textbf{Machine Learning} is a set of tools that enable computationally-feasible data-driven search over high-dimensional functional spaces.}
        }}
    \end{center}
\end{frame}

\begin{frame}{A General Framework}
\begin{equation*}
    y = f(\boldsymbol{x}) + \varepsilon
\end{equation*}
\begin{itemize}
    \item $y \in \mathbb{R}^k$ is some "target" or "outcome";
    \item $\boldsymbol{x} \in \R^p$ is a vector of "features", or "predictors", or "covariates";
    \item $f: \R^p \to \R^k$ is some unknown function;
    \item $\varepsilon$ is some unobserved noise because the world is messy;
\end{itemize}
\pause
\vspace{0.5cm}
\textbf{Question:} given a function space $\mathcal{F}$, how to find $\hat{f} \in \mathcal{F}$ that approximates $f$ well?
\pause
\begin{itemize}
    \item Collect data $\{(y_1, \boldsymbol{x}_1), \ldots, (y_n, \boldsymbol{x}_n)\}$;
    \item Define some notion of "approximates well" $\implies$ (a loss function);
    \item Be explicit about $\mathcal{F}$;
    \item Be explicit about your optimization mechanism;
\end{itemize}
\end{frame}

\begin{frame}{You are already doing ML!}
Consider an outcome $y_i$, and a set of covariates $\boldsymbol{x}_i$ for $i=1,\ldots,n$:
\begin{equation*}
    y_i = \alpha + \boldsymbol{x}_i'\boldsymbol{\beta} + \varepsilon_i
\end{equation*}
\begin{itemize}
    \item This is a linear regression model;
    \item The function space $\mathcal{F}$ is the set of all affine functions of the treatment and covariates;
    \item The loss function is the MSE: $\mathcal{L}(y_i, \hat{y}_i) = (y_i - \hat{y}_i)^2$;
    \item OLS: minimize a convex loss function over the space of parameters;
\end{itemize}
\pause
\vspace{0.5cm}
\textbf{Conclusion:} Linear regression is a (very simple) ML method! But there is so much more...
    
\end{frame}

\begin{frame}{Hold on... Isn't Machine Learning just Non-Parametric Estimation?}
\begin{itemize}
    \item The general framework I used could be used in a non-parametric estimation class...
    \item Why do we need new tools? We already have the good and old kernel regression!
    \pause
    \item Well... there is the curse of dimensionality! If $p \approx 6$, you are already in trouble!
\end{itemize}
\pause
\vspace{0.2cm}
\begin{columns}[T,onlytextwidth]
    \column{0.46\textwidth}
    \vspace{-0.1cm}
    \begin{center}
        \textbf{OLS}
    \end{center}
    \vspace{-0.2cm}
    \begin{itemize}
        \item[\textcolor{Red}{\bullet}] Leverages linearity (strong!);
        \item[\textcolor{ForestGreen}{\bullet}] Easy to compute and interpret;
    \end{itemize}
    \column{0.08\textwidth}
    \centering
    {\color{black}\rule{0.8pt}{0.3\textheight}}
    \column{0.46\textwidth}
    \vspace{-0.1cm}
    \begin{center}
        \textbf{Fully Non-Parametric Methods}
    \end{center}
    \vspace{-0.2cm}
    \begin{itemize}
        \item[\textcolor{ForestGreen}{\bullet}] Extreme flexibility;
        \item[\textcolor{Red}{\bullet}] Super data hungry!
    \end{itemize}
\end{columns}
\vspace{0.5cm}
\fbox{\parbox{0.95\textwidth}{\centering
        \textcolor{FGVBlue}{Machine Learning = a \textit{compromise}: richer parametrizations while still computationally feasible in high dimensions.}
        }}
\end{frame}

\begin{frame}[standout]
    Questions?
\end{frame}

\section{Causality in High Dimensions}

\begin{frame}{Preliminaries}
\begin{itemize}\itemsep0.5em
    \item No estimator will lead to causality by itself -- only careful design will;
    \pause
    \item ML methods were \textit{not} created to tackle causality problems;
    \item See {\small \cite{varian_big_2014}, \cite{mullainathan_machine_2017}, \cite{athey_machine_2019}, and \cite{masini_machine_2023}};
    \pause
    \item \cite{kleinberg_prediction_2015}: many policy-relevant questions are prediction problems!
    \pause
    \item Belloni \emoji{flag-brazil}, Chernozhukov, Hansen and co-authors took it even further:
    \begin{itemize}
        \item Computing the propensity score \textit{is} forecasting!
        \item The first-stage regression in an IV context \textit{is} forecasting!
    \end{itemize}
\end{itemize}
    
\end{frame}

% The General Problem
\begin{frame}{Treatment Effects in High Dimensions}
Suppose you're interested in the treatment effect $\theta_0 \in \mathbb{R}$:
\begin{equation*}
    y_i = d_i \theta_0 + \boldsymbol{x}_i'\boldsymbol{\beta} + \varepsilon_i
\end{equation*}
\begin{itemize}
    \item $y_i \in \mathbb{R}$ is an outcome;
    \item $d_i \in \mathbb{R}$ is a treatment;
    \item $\boldsymbol{x}_i \in \R^p$ is a vector of available covariates;
    \item $\varepsilon_i$ is some unobserved noise with $\E[\varepsilon_i \mid d_i, \boldsymbol{x}_i] = 0$;
    \pause
    \item You have an i.i.d. sample $\{y_i, d_i, \boldsymbol{x}_i\}_{i=1}^n$ and we \underline{allow} for $p \gg n$;
    \pause
    \item \textbf{Goal}: estimate $\theta_0$ and get a confidence interval;
\end{itemize}
\pause
\vspace{0.5cm}
\textbf{Question}: what will happen if you try OLS here?
\end{frame}

% Lasso 2
\begin{frame}{Treatment Effects in High Dimensions}
\begin{itemize}
    \item Let's say you believe only a few $\beta_j$'s are $\neq 0$ $\implies$ \textit{``sparsity''} in $\boldsymbol{\beta}$;
    \item But you do not know which ones!
    \pause
    \item What about using your economic intuition to select a subset of controls?
    \pause
    \item Applying Econ theory is always a good idea, but:
    \begin{itemize}
        \item You might not get a meaningful reduction with theory alone;
        \item Your referee might not agree with your choices;
        \item You might get lost in a sea of robustness checks...
    \end{itemize}
    \pause
    \item Good news: ML researchers devoted a lot of attention to \textit{sparse regressions}!
\end{itemize}
\end{frame}

\begin{frame}{Welcome to SBE, Mr. LASSO}
The Least Absolute Shrinkage and Selection Operator (LASSO) estimator solves:
\begin{equation*}
    \hat{\boldsymbol{\delta}} \equiv \arg \min_{\boldsymbol{\delta} \in \R^p} \left\{ \frac{1}{n} \sum_{i=1}^n (y_i - \boldsymbol{w}_i'\boldsymbol{\delta})^2 + \lambda \sum_{j=1}^p |\delta_j| \right\}
\end{equation*}
\begin{itemize}
    \item $\lambda \geq 0$ is a tuning parameter that controls the amount of penalization (``\textit{regularization}'');
    \item $\boldsymbol{w}_i$ is a general vector of regressors of size $p$;
    \pause
    \item The $\ell_1$ penalty $\sum_{j=1}^p |\delta_j|$ induces sparsity in $\hat{\boldsymbol{\delta}}$;
    \pause
    \item If $\lambda = 0$, we get OLS; if $\lambda \to \infty$, we get $\hat{\boldsymbol{\delta}} = \boldsymbol{0}$;
    \pause
    \item For intermediate values of $\lambda$, some $\hat{\delta}_j$'s will be exactly zero!
    \pause
    \item $\hat{\boldsymbol{\delta}}$ is gives up unbiasedness for much lower variance;
    \item This problem is still feasible if $p \gg n$ and it is convex $\implies$ fast computation;
\end{itemize}
\end{frame}

\begin{frame}{The Geometry of LASSO}
\begin{columns}[T,onlytextwidth]
    \begin{column}{0.55\textwidth}
        For $c > 0$, consider the following:
        \begin{gather*}
            \tilde{\boldsymbol{\delta}} \equiv \arg \min_{\boldsymbol{\delta} \in \R^p} \left\{ \frac{1}{n} \sum_{i=1}^n (y_i - \boldsymbol{w}_i'\boldsymbol{\delta})^2 \right\} \\
            \text{ subject to } \sum_{j=1}^p |\delta_j| \leq c
        \end{gather*}
        \begin{itemize}
            \item Think about the Lagrangian of this problem!
            \item For every $\lambda$, there is a $c$ such that $\hat{\boldsymbol{\delta}} = \tilde{\boldsymbol{\delta}}$;
        \end{itemize}
    \end{column}

    \begin{column}{0.45\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{code/lasso_geometry_delta.png}
        \end{center}
    \end{column}
\end{columns}
\end{frame}

\begin{frame}{Generalizations}
    
\end{frame}


% --- Appendix ---
\appendix

\begin{frame}[standout]
  \begin{center}
    \Huge
    Appendix and References
  \end{center}
\end{frame}

\begin{frame}[allowframebreaks]{References}
    \printbibliography[heading=none]
\end{frame}

\end{document}
