\documentclass[aspectratio=169, xcolor=dvipsnames, 11pt]{beamer}
\usetheme{moloch}
\molochset{
  progressbar = frametitle,
  block = fill
}
% Use beamer's page number in head/foot template
\setbeamertemplate{footline}[frame number]
\setbeamersize{text margin left=0.5cm,text margin right=0.5cm}

% from https://tex.stackexchange.com/a/441705/36296
\makeatletter
\addtobeamertemplate{date}{\centering}{}
\addtobeamertemplate{institute}{\centering}{}
\patchcmd{\beamer@@tmpl@title}{\raggedright}{\centering}{}{}
\patchcmd{\beamer@@tmpl@author}{\raggedright}{\centering}{}{}
\setlength{\moloch@titleseparator@linewidth}{2pt}
\setlength{\moloch@progressinheadfoot@linewidth}{1.5pt}
\setbeamercolor{progress bar}{fg=FGVBlue, bg=white}
\makeatother

% ----- Packages -----
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{xcolor} % For colored bullets
\usepackage{appendixnumberbeamer}
\usepackage{emoji}
\setemojifont{Apple Color Emoji}
% TikZ for decorative braces in slides
\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing,positioning,calc}
\usepackage[normalem]{ulem}
\renewcommand{\ULthickness}{.20ex}

% ----- Metadata -----
\title[]{ML in Economics and Finance: Where do We Go Now? - Part I}

\author[]{
  \textbf{Raul Riva}
}
\institute[]{
  \textcolor{FGVBlue}{FGV EPGE}
  \vspace{0.75cm}
}
\date[]{December, 2025 \\ \vspace{1cm} INSPER - SÃ£o Paulo}

% ----- Custom commands -----
\newcommand{\R}{{\mathbb{R}}}
\renewcommand{\Pr}{{\mathbb{P}}}
\newcommand{\E}{{\mathbb{E}}}
\newcommand{\var}{{\mathrm{var}}}
\newtheorem{prop}[theorem]{Proposition}

% ----- Defining colors -----
\usepackage[dvipsnames]{xcolor}
\definecolor{slateblue}{RGB}{45, 62, 80}
\definecolor{KelloggPurple}{HTML}{4E2A84}
\setbeamercolor{palette primary}{bg=slateblue, fg=white}
\definecolor{FGVBlue}{RGB}{0, 114, 188}
\setbeamercolor{title separator}{bg=FGVBlue, fg=FGVBlue}
\setbeamercolor{background canvas}{bg=white}

% ----- Hyperlinks and Bibliography -----
\usepackage[backend=biber,style=authoryear,maxcitenames=2,mincitenames=1,maxbibnames=99,uniquename=false,uniquelist=false]{biblatex}
\ExecuteBibliographyOptions{maxcitenames=2,mincitenames=1,uniquelist=false}
\addbibresource{../references.bib}
\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=FGVBlue,
  urlcolor=FGVBlue,
  citecolor=FGVBlue
} % close \hypersetup
\DeclareFieldFormat{labelyear}{#1}
\DeclareFieldFormat{shorthand}{#1}
\DeclareFieldFormat{extraalpha}{#1}
\AtEveryCitekey{\color{FGVBlue}}
\DeclareNameFormat{labelname}{%
    \nameparts{#1}%
    \usebibmacro{name:family}{\namepartfamily}{\namepartgiven}{\namepartprefix}{\namepartsuffix}%
    \usebibmacro{name:andothers}}
\let\cite\textcite
\renewcommand*{\bibfont}{\tiny}

% Make section titles slateblue
\makeatletter
% Preserve starred sections and optional arguments while coloring section titles.
\let\oldsection\section
\renewcommand{\section}{\@ifstar\star@section\@section}
\newcommand{\@section}[2][]{%
    \if\relax\detokenize{#1}\relax
        \oldsection{\textcolor{slateblue}{#2}}%
    \else
        \oldsection[#1]{\textcolor{slateblue}{#2}}%
    \fi
}
\newcommand{\star@section}[1]{\oldsection*{\textcolor{slateblue}{#1}}}
\makeatother


%---------------- Document Start ----------------%
\begin{document}

%---------------- Title Slide ----------------%
% Title page without margin changes
\begin{frame}[plain] % Use plain to avoid navigation symbols
  \titlepage
\end{frame}

\section{Intro}
%---------------- Intro ----------------%
\begin{frame}{Who is this guy?}
    \begin{itemize}\itemsep1em
        \item Just joined \textcolor{FGVBlue}{FGV EPGE} as an Assistant Professor;
        \item PhD in Finance from \textcolor{KelloggPurple}{Northwestern University};
        \item Asset Pricing + Macro-Finance + Econometrics;
        \end{itemize}
        \pause
        \vspace{1cm}
        \begin{center}
        \fbox{\parbox{0.9\textwidth}{\centering
        \alert{I am \textbf{not} an ML developer, but maybe a mildly sophisticated consumer}
        }}
        \end{center}
\end{frame}

\begin{frame}{Where are we?}
    \begin{itemize}\itemsep0.6em
            \item Last 20-30 years: explosion of computational power and popularization of ML techniques;
            \item Last 15 years: we economists imported several techniques from CS and Stats;
            \pause
            \item Many challenges in this translation:
            \vspace{-0.5cm}
                \begin{columns}[t]
                    \begin{column}{0.44\textwidth}
                        \begin{itemize}\itemsep0.3em
                            \item Causality vs pattern recognition;
                            \item Interpretability;
                        \end{itemize}
                    \end{column}
                    \begin{column}{0.44\textwidth}
                        \begin{itemize}\itemsep0.5em
                            \item Sophisticated notions of equilibrium;
                            \item Time series dynamics;
                        \end{itemize}
                    \end{column}
                \end{columns}
        \end{itemize}
    \pause
    \vspace{0.75cm}
    \textbf{Right now}:
    \begin{itemize}\itemsep0.6em
        \item Better understanding of the limitations of "plug and play" ML;
        \item[\textcolor{ForestGreen}{\bullet}] Great stuff: new hybrid methods designed by and for economists;
        \item[\textcolor{Red}{\bullet}] Bad stuff: we are flooded with tutorials, books, videos, bootcamps...
    \end{itemize}   
\end{frame}

\begin{frame}{Where do we go now?}
    \begin{itemize}
        \item The Econ/Finance forecasting crowd was really fast in adopting ML...
        \item But what else? What is worth knowing about ML in Econ and Finance? What's the frontier?
    \end{itemize}
\pause
\vspace{0.5cm}
    \textbf{What I will do}:
    \begin{itemize}
        \item My own economist-crafted definition of ML methods and how to think about them;
        \item Three very cool agendas where ML can help economists;
        \item Causality in HD, seriously heterogeneous treatment effects, and solving large models;
    \end{itemize}
\pause
\vspace{0.5cm}
    \textbf{What I will \alert{\underline{not}} do}:
    \begin{itemize}
        \item Teach you how to code;
        \pause
        \item Walk you through proofs and be super formal;
        \pause
        \item Lie to you and say you can easily perform any of this in Stata! \emoji{face-with-rolling-eyes} 
    \end{itemize}
\end{frame}

\begin{frame}{Who is this for?}
    \begin{itemize}
        \item Students starting their empirical research agendas;
        \item Fellow empirical researchers trying to grasp what kind of ML tools might be useful;
        \item Someone coming from Stats or CS into Economics;
    \end{itemize}
\pause
\vspace{0.5cm}
\textbf{Who is this \textcolor{red}{\underline{not}} for?}
\begin{itemize}
    \item Super sophisticated economists already deploying these techniques everywhere;
    \item Hardcore econometricians looking for \sout{\textit{dark magic}} ultra fancy theorems and proofs;
\end{itemize}
\pause
\vspace{0.5cm}
\begin{center}
        \fbox{\parbox{0.99\textwidth}{\centering
        DISCLAIMER: These are \textbf{my} own views, based on \textbf{my} experience, and \textbf{my} own readings.\\ Other people will disagree.
        }}
\end{center}

\end{frame}

\begin{frame}{Flight Plan {\normalfont\emoji{airplane}}}
    \begin{enumerate}
        \item What is ML, anyway?\tikz[remember picture,baseline] \node (it1) {};
        \item Causality in High Dimensions\tikz[remember picture,baseline] \node (it2) {};
        \item (Seriously) Heterogeneous Partial Effects\tikz[remember picture,baseline] \node (it3) {};
        \item Solving Large-Scale General Equilibrium Models\tikz[remember picture,baseline] \node (it4) {};
    \end{enumerate}

    % Decorative braces indicating Today / Tomorrow on the right
    \begin{tikzpicture}[remember picture,overlay]
        \coordinate (todayAnchor) at ($(it2.east)+(0.45cm,-0.2cm)$);
        \coordinate (tomorrowAnchor) at ($(it4.east)+(0.45cm,-0.2cm)$);

        \draw[decorate,decoration={brace,amplitude=6pt,mirror},line width=1pt]
            (todayAnchor |- it2.south) -- ($(todayAnchor |- it1.north)+(0,0.3cm)$)
            node[midway,right=12pt]{\textbf{Today}};

        \draw[decorate,decoration={brace,amplitude=6pt,mirror},line width=1pt]
            (tomorrowAnchor |- it4.south) -- ($(tomorrowAnchor |- it3.north)+(0,0.3cm)$)
            node[midway,right=12pt]{\textbf{Tomorrow}};
    \end{tikzpicture}
    \pause
    \vspace{0.5cm}
    \begin{center}
        	\textbf{\textcolor{FGVBlue}{Please bring questions at any time!}}
    \end{center}
\end{frame}

\begin{frame}[standout]
    Questions?
\end{frame}

\section{A General Framework}
\begin{frame}{What is \textit{Machine Learning}?}
    \begin{itemize}\itemsep0.6em
        \item Different fields = different definitions: CS, Stats, Operations Research, ...
        \item Many types: Supervised, Unsupervised, Reinforcement Learning, ...
        \item More buzzwords = better consulting gigs! \emoji{money-mouth-face}
        \pause
        \item Today and tomorrow: \textbf{Supervised Learning};
        \pause
        \item I will be brave enough and provide the one I think is really useful for Economists:
    \end{itemize}
    \pause
    \vspace{0.6cm}
    \begin{center}
        \fbox{\parbox{0.95\textwidth}{\centering
        \alert{(Supervised) \textbf{Machine Learning} is a set of tools that enable computationally-feasible data-driven search over high-dimensional functional spaces.}
        }}
    \end{center}
\end{frame}

\begin{frame}{A General Framework}
\begin{equation*}
    y = f(\boldsymbol{x}) + \varepsilon
\end{equation*}
\begin{itemize}
    \item $y \in \mathbb{R}^k$ is some "target" or "outcome";
    \item $\boldsymbol{x} \in \R^p$ is a vector of "features", or "predictors", or "covariates";
    \item $f: \R^p \to \R^k$ is some unknown function;
    \item $\varepsilon$ is some unobserved noise because the world is messy;
\end{itemize}
\pause
\vspace{0.5cm}
\textbf{Question:} given a function space $\mathcal{F}$, how to find $\hat{f} \in \mathcal{F}$ that approximates $f$ well?
\pause
\begin{itemize}
    \item Collect data $\{(y_1, \boldsymbol{x}_1), \ldots, (y_n, \boldsymbol{x}_n)\}$;
    \item Define some notion of "approximates well" $\implies$ (a loss function);
    \item Be explicit about $\mathcal{F}$;
    \item Be explicit about your optimization mechanism;
\end{itemize}
\end{frame}

\begin{frame}{You are already doing ML!}
Consider an outcome $y_i$, and a set of covariates $\boldsymbol{x}_i$ for $i=1,\ldots,n$:
\begin{equation*}
    y_i = \alpha + \boldsymbol{x}_i'\boldsymbol{\beta} + \varepsilon_i
\end{equation*}
\begin{itemize}
    \item This is a linear regression model;
    \item The function space $\mathcal{F}$ is the set of all affine functions of the treatment and covariates;
    \item The loss function is the MSE: $\mathcal{L}(y_i, \hat{y}_i) = (y_i - \hat{y}_i)^2$;
    \item OLS: minimize a convex loss function over the space of parameters;
\end{itemize}
\pause
\vspace{0.5cm}
\textbf{Conclusion:} Linear regression is a (very simple) ML method! But there is so much more...
    
\end{frame}

\begin{frame}{Hold on... Isn't Machine Learning just Non-Parametric Estimation?}
\begin{itemize}
    \item The general framework I used could be used in a non-parametric estimation class...
    \item Why do we need new tools? We already have the good and old kernel regression!
    \pause
    \item Well... there is the curse of dimensionality! If $p \approx 6$, you are already in trouble!
\end{itemize}
\pause
\vspace{0.2cm}
\begin{columns}[T,onlytextwidth]
    \column{0.46\textwidth}
    \vspace{-0.1cm}
    \begin{center}
        \textbf{OLS}
    \end{center}
    \vspace{-0.2cm}
    \begin{itemize}
        \item[\textcolor{Red}{\bullet}] Leverages linearity (strong!);
        \item[\textcolor{ForestGreen}{\bullet}] Easy to compute and interpret;
    \end{itemize}
    \column{0.08\textwidth}
    \centering
    {\color{black}\rule{0.8pt}{0.3\textheight}}
    \column{0.46\textwidth}
    \vspace{-0.1cm}
    \begin{center}
        \textbf{Fully Non-Parametric Methods}
    \end{center}
    \vspace{-0.2cm}
    \begin{itemize}
        \item[\textcolor{ForestGreen}{\bullet}] Extreme flexibility;
        \item[\textcolor{Red}{\bullet}] Super data hungry!
    \end{itemize}
\end{columns}
\vspace{0.5cm}
\pause
\fbox{\parbox{0.95\textwidth}{\centering
        \textcolor{FGVBlue}{Machine Learning = a \textit{compromise}: richer parametrizations while still computationally feasible in high dimensions.}
        }}
\end{frame}

\begin{frame}[standout]
    Questions?
\end{frame}

\section{Causality in High Dimensions}

\begin{frame}{Preliminaries}
\begin{itemize}\itemsep0.5em
    \item No estimator will lead to causality by itself -- only careful design will;
    \pause
    \item ML methods were \textit{not} created to tackle causality problems;
    \item See {\small \cite{varian_big_2014}, \cite{mullainathan_machine_2017}, \cite{athey_machine_2019}, and \cite{masini_machine_2023}};
    \pause
    \item \cite{kleinberg_prediction_2015}: many policy-relevant questions are prediction problems!
    \pause
    \item Belloni \emoji{flag-brazil}, Chernozhukov, Hansen and co-authors took it even further:
    \begin{itemize}
        \item Computing the propensity score \textit{is} forecasting!
        \item The first-stage regression in an IV context \textit{is} forecasting!
    \end{itemize}
\end{itemize}
    
\end{frame}

% The General Problem
\begin{frame}{Treatment Effects in High Dimensions}
Suppose you're interested in the treatment effect $\theta_0 \in \mathbb{R}$:
\begin{equation*}
    y_i = d_i \theta_0 + \boldsymbol{x}_i'\boldsymbol{\beta} + \varepsilon_i
\end{equation*}
\begin{itemize}
    \item $y_i \in \mathbb{R}$ is an outcome;
    \item $d_i \in \mathbb{R}$ is a treatment;
    \item $\boldsymbol{x}_i \in \R^p$ is a vector of available covariates;
    \item $\varepsilon_i$ is some unobserved noise with $\E[\varepsilon_i \mid d_i, \boldsymbol{x}_i] = 0$;
    \pause
    \item You have an i.i.d. sample $\{y_i, d_i, \boldsymbol{x}_i\}_{i=1}^n$ and we \underline{allow} for $p \gg n$;
    \pause
    \item \textbf{Goal}: estimate $\theta_0$ and get a confidence interval;
\end{itemize}
\pause
\vspace{0.5cm}
\textbf{Question}: what will happen if you try OLS here?
\end{frame}

% Lasso 2
\begin{frame}{Treatment Effects in High Dimensions}
\begin{itemize}\itemsep0.7em
    \item Let's say you believe only a few $\beta_j$'s are $\neq 0$ $\implies$ \textit{``sparsity''} in $\boldsymbol{\beta}$;
    \item But you do not know which ones!
    \pause
    \item What about using your economic intuition to select a subset of controls?
    \pause
    \item Applying Econ theory is always a good idea, but:
    \begin{itemize}
        \item You might not get a meaningful reduction with theory alone;
        \item Your referee might not agree with your choices;
        \item You might get lost in a sea of robustness checks...
    \end{itemize}
    \pause
    \item Good news: ML researchers devoted a lot of attention to \textit{sparse regressions}!
\end{itemize}
\end{frame}

\begin{frame}{Welcome to SBE, Mr. LASSO}
The Least Absolute Shrinkage and Selection Operator (LASSO) estimator solves:
\begin{equation*}
    \hat{\boldsymbol{\delta}} \equiv \arg \min_{\boldsymbol{\delta} \in \R^p} \left\{ \frac{1}{n} \sum_{i=1}^n (y_i - \boldsymbol{w}_i'\boldsymbol{\delta})^2 + \lambda \sum_{j=1}^p |\delta_j| \right\}
\end{equation*}
\begin{itemize}
    \item $\lambda \geq 0$ is a tuning parameter that controls the amount of penalization (``\textit{regularization}'');
    \item $\boldsymbol{w}_i$ is a general vector of regressors of size $p$;
    \pause
    \item The $\ell_1$ penalty $\sum_{j=1}^p |\delta_j|$ induces sparsity in $\hat{\boldsymbol{\delta}}$;
    \pause
    \item If $\lambda = 0$, we get OLS; if $\lambda \to \infty$, we get $\hat{\boldsymbol{\delta}} = \boldsymbol{0}$;
    \pause
    \item For intermediate values of $\lambda$, some $\hat{\delta}_j$'s will be exactly zero!
    \pause
    \item $\hat{\boldsymbol{\delta}}$ is gives up unbiasedness for much lower variance;
    \item This problem is still feasible if $p \gg n$ and it is convex $\implies$ fast computation;
\end{itemize}
\end{frame}

\begin{frame}{The Geometry of LASSO}
\begin{columns}[T,onlytextwidth]
    \begin{column}{0.55\textwidth}
        For $c > 0$, consider the following:
        \begin{gather*}
            \tilde{\boldsymbol{\delta}} \equiv \arg \min_{\boldsymbol{\delta} \in \R^p} \left\{ \frac{1}{n} \sum_{i=1}^n (y_i - \boldsymbol{w}_i'\boldsymbol{\delta})^2 \right\} \\
            \text{ subject to } \sum_{j=1}^p |\delta_j| \leq c
        \end{gather*}
        \begin{itemize}
            \item Think about the Lagrangian of this problem!
            \item For every $\lambda$, there is a $c$ such that $\hat{\boldsymbol{\delta}} = \tilde{\boldsymbol{\delta}}$;
        \end{itemize}
    \end{column}

    \begin{column}{0.45\textwidth}
        \begin{center}
            \includegraphics[width=0.95\textwidth]{../pictures/lasso_geometry_delta.png}
        \end{center}
    \end{column}
\end{columns}
\end{frame}

\begin{frame}{Exploring Options}
Recall our treatment effects model:
\begin{equation*}
    y_i = d_i \theta_0 + \boldsymbol{x}_i'\boldsymbol{\beta} + \varepsilon_i
\end{equation*}
\begin{itemize}
    \item \textbf{Approach 1}: run LASSO of $y_i$ on $d_i$ and $\boldsymbol{x}_i$. Is this a good idea?
    \pause 
    \item $\hat{\theta}_0$ will be severely biased and might even be set to zero!
    \vspace{0.3cm}
    \pause
    \item \textbf{Approach 2}: run LASSO but with no penalty on $\theta_0$. Is this a good idea?
    \pause
    \item Still terrible! $\hat{\theta}_0$ will still be biased!
    \pause
    \vspace{0.3cm}
    \item \textbf{Approach 3}: do Approach 2, but then run \underline{OLS} on the selected $x_i$ and $d_i$. Is this good?
    \pause
    \item  \cite{leeb_can_2008} and \cite{leeb_guest_2008}: terrible idea again!
    \item Main problem: \textit{omitted variable bias} if some relevant controls are not selected!
    \item If some $x_j$ is correlated with $d_i$ and affects $y_i$, omitting it biases $\hat{\theta}_0$!
\end{itemize}
\end{frame}

\begin{frame}{Something That Finally Works!}
\cite{belloni_inference_2014} thought about how $d_i$ and $\boldsymbol{x}_i$ interact:
\begin{equation*}
   d_i = \boldsymbol{x}_i'\boldsymbol{\gamma} + u_i, \quad \E[u_i \mid \boldsymbol{x}_i] = 0
\end{equation*}
\begin{itemize}
    \item What if $\boldsymbol{\gamma}$ is also sparse, i.e., only a few $x_j$'s affect $d_i$?
    \item Can we find a small subset of $\boldsymbol{x}_i$ that \textit{predicts} treatment well?
\end{itemize}
  \pause

\vspace{0.5cm}
They proposed the \textbf{Double LASSO} procedure:
\begin{enumerate}
    \item Run LASSO of $y_i$ on $\boldsymbol{x}_i$ to select controls $\hat{S}_y$;
    \item Run LASSO of $d_i$ on $\boldsymbol{x}_i$ to select controls $\hat{S}_d$;
    \item Run OLS of $y_i$ on $d_i$ and $\boldsymbol{x}_i$ with $x_j \in \hat{S}_y \cup \hat{S}_d$;
\end{enumerate}
\end{frame}

\begin{frame}{A Really Cool Result}
\cite{belloni_high-dimensional_2014} provide conditions under which:
\begin{equation*}
    \sqrt{n}(\hat{\theta}_0 - \theta_0) \xrightarrow{d} \mathcal{N}(0, \sigma^2)
\end{equation*}
where $\sigma^2$ is complicated but consistently estimated.
\pause
\vspace{0.5cm}

\textbf{The impressive stuff}:
\begin{itemize}
    \item This convergence is uniform over a large class of DGPs;
    \item Convergence still happens at the rate $\sqrt{n}$, even if $p \gg n$!
    \item Under homoskedasticity, it attains semi-parametric efficiency!
    \item Construct confidence intervals in the usual ways;
\end{itemize}
\vspace{0.4cm}
\textbf{Key assumption}: sparse representation;
\end{frame}

\begin{frame}{Some Monte-Carlo Reassurance}
\begin{figure}
    \centering
    \includegraphics[width=0.95\textwidth]{../pictures/Single_vs_Double_Selection.png}
\end{figure}
\end{frame}

\begin{frame}[standout]
    Questions?
\end{frame}

\section{Limitations and Generalizations}

\begin{frame}{Limitations}
\begin{itemize}
    \item What if we want to allow for non-linearities?
    \item What if we want to use other ML methods?
    \item What if sparsity is not a good assumption?
    \item What if treatment has heterogenous effects?
    \item What if outcomes are function-valued?
\end{itemize}
\end{frame}

\begin{frame}{Generalization}
\cite{belloni_program_2017} and \cite{chernozhukov_doubledebiased_2018} generalize all of this:
\begin{gather*}
    y_i = g_0(d_i, \boldsymbol{x}_i) + \varepsilon_i, \quad \E[\varepsilon_i \mid d_i, \boldsymbol{x}_i] = 0 \\
    d_i = m_0(\boldsymbol{x}_i) + u_i, \quad \E[u_i \mid \boldsymbol{x}_i] = 0
\end{gather*}

\begin{itemize}
    \item $g_0(\cdot)$ and $m_0(\cdot)$ are unknown (possibly non-linear) functions;
    \item You can use several different ML method to estimate $g_0(\cdot)$ and $m_0(\cdot)$;
    \item Sparsity is not necessary anymore;
    \pause
    \item Secrete sauce I: \underline{Neyman Orthogonal Scores} $\psi$
    \[
    \E\left[\psi(\text{data}, \underbrace{\text{param of interest}}_{\equiv \theta_0}, \underbrace{\text{nuisance params}}_{\equiv \eta_0})\right] = 0, \qquad \frac{\partial}{\partial \eta} \E\left[\psi(\text{data}, \theta_0, \eta)\right] \Big|_{\eta = \eta_0} = 0
    \]
    \pause
    \item Secrete sauce II: \underline{cross-fitting} $\implies$ efficiency vs strict assumptions;
    \pause
    \item Independence across $i$ is \underline{essential};
\end{itemize}
\end{frame}

\begin{frame}{A Concrete Example (A Partially Linear Model)}
    \begin{gather*}
    y_i = d_i\theta_0 + g_0(\boldsymbol{x}_i) + \varepsilon_i, \quad \E[\varepsilon_i \mid d_i, \boldsymbol{x}_i] = 0 \\
    d_i = m_0(\boldsymbol{x}_i) + u_i, \quad \E[u_i \mid \boldsymbol{x}_i] = 0
\end{gather*}
\pause
Steps:
\begin{itemize}
    \item Divide the data into two folds;
    \item On fold 1, estimate $\hat{g}_0(\boldsymbol{x}_i)$ and $\hat{m}_0(\boldsymbol{x}_i)$ using ML methods;
    \item On fold 2, compute residuals:
    \begin{gather*}
        \hat{\varepsilon}_i = y_i - \hat{g}_0(\boldsymbol{x}_i) \\
        \hat{u}_i = d_i - \hat{m}_0(\boldsymbol{x}_i)
    \end{gather*}
    \item Regress $\hat{\varepsilon}_i$ on $\hat{u}_i$ to get $\hat{\theta}_0$;
    \pause
    \item Repeat switching folds and average $\hat{\theta}_0$'s;
    \pause
    \item In practice you can use $K$ folds!
    \item See \cite{chernozhukov_doubledebiasedneyman_2017} for a practical guide!
\end{itemize}

    
\end{frame}

\begin{frame}{Where do we go now?}
Some open problems:
\begin{itemize}
    \item Weak identification, in special in the IV context (see \cite{Buhlmann2025});
    \item Time series $\implies$ it's impossible to do cross-fitting (see \cite{NEURIPS});
    \item Panel data $\implies$ usual estimators leverage linearity (see \cite{Chernozhukov2021} and \cite{Clarke2025});
\end{itemize}
\pause
\vspace{1cm}
\begin{center}
    \alert{Good news! Plenty of dissertation topics!}
\end{center}
\end{frame}

\begin{frame}[standout]
    Questions?
\end{frame}

\begin{frame}[standout]
    \begin{center}
       Thank you!\\
        See you tomorrow, stay tuned! \\
        \emoji{call-me-hand}
    \end{center}
\end{frame}



% --- Appendix ---
\appendix

\begin{frame}[standout]
  \begin{center}
    \Huge
    Appendix and References
  \end{center}
\end{frame}

\begin{frame}{References}
    \footnotesize
    \printbibliography[heading=none]
\end{frame}

\end{document}
